{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a53f06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# loading libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "from PIL import Image\n",
    "\n",
    "# import tensorflow and keras packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# import PCA adn tSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b1a82",
   "metadata": {},
   "source": [
    "#### PCA and Linear Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0832c59",
   "metadata": {},
   "source": [
    "We can compare PCA with a simple linear autoencoder and in doing so, we can understand the syntax for an autoencoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a7eeb",
   "metadata": {},
   "source": [
    "We will utilize the MNIST digits data again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b947d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# we will convert our images into 1x784 vectors\n",
    "X_train = X_train.reshape(-1, 28*28).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(-1, 28*28).astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41735a",
   "metadata": {},
   "source": [
    "We don't need the digit labels for autoencoders - these are \"almost\" unsupervised models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7c81f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dimensions for latent space\n",
    "latent_dim = 64 # 1x64 64 numbers in a vector will be used to compress our inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300d1e2d",
   "metadata": {},
   "source": [
    "Let's apply PCA to the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e9f4de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=latent_dim) # generate 64 principal components\n",
    "X_train_pca = pca.fit_transform(X_train) # learn representation on training input\n",
    "X_test_pca = pca.transform(X_test) # apply to test input\n",
    "\n",
    "X_test_pca_reconstructed = pca.inverse_transform(X_test_pca) # reconstruct input "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6f081",
   "metadata": {},
   "source": [
    "Let's define our autoencoder. Recall that an autoencoder network is built on Encoder and Decoder blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input\n",
    "input_img = ...\n",
    "\n",
    "# encoder (linear)\n",
    "encoded = ...\n",
    "\n",
    "# decoder (linear)\n",
    "decoded = ...\n",
    "\n",
    "# the entire model\n",
    "linear_autoencoder = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b01caf",
   "metadata": {},
   "source": [
    "Let's compile and train our linear autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad546ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_autoencoder.compile(optimizer=tf.keras.optimizers.Adam(1e-3),loss=\"mse\")\n",
    "\n",
    "linear_autoencoder.fit(X_train,X_train, # input and output are the same\n",
    "                       epochs=20,\n",
    "                       batch_size=128,\n",
    "                       shuffle=True,\n",
    "                       validation_data=(X_test,X_test),\n",
    "                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513938c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear autoencoder model architecture\n",
    "linear_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9203c7",
   "metadata": {},
   "source": [
    "Let's now reconstruct our inputs using the autoencoder and compare with the PCA reconstructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test images\n",
    "X_test_ae_reconstructed = linear_autoencoder.predict(X_test)\n",
    "\n",
    "# visualization function \n",
    "def plot_reconstructions(input,pca_output,ae_output,n=10):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        ax = plt.subplot(3, n, i + 1)\n",
    "        plt.imshow(input[i].reshape(28, 28), cmap=\"gray\")\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # PCA\n",
    "        ax = plt.subplot(3, n, i + 1 + n)\n",
    "        plt.imshow(pca_output[i].reshape(28, 28), cmap=\"gray\")\n",
    "        plt.title(\"PCA\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Autoencoder\n",
    "        ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
    "        plt.imshow(ae_output[i].reshape(28, 28), cmap=\"gray\")\n",
    "        plt.title(\"Linear AE\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# pass in parameters to the function\n",
    "plot_reconstructions(X_test, X_test_pca_reconstructed, X_test_ae_reconstructed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9809e3",
   "metadata": {},
   "source": [
    "The reconstructions are almost identical. This adds weight to the notion that linear autoencoders are a form of PCA technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10e9ef",
   "metadata": {},
   "source": [
    "#### Deep Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd1b43",
   "metadata": {},
   "source": [
    "Let's now train a deep non-linear autoencoder. The expectation here is that the latent space will learn a better representation of the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f0999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input\n",
    "input_img = ...\n",
    "\n",
    "# encoder (linear)\n",
    "encoded = ...\n",
    "\n",
    "# decoder (linear)\n",
    "decoded = ...\n",
    "\n",
    "# the entire model\n",
    "deep_autoencoder = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e83b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_autoencoder.compile(optimizer=tf.keras.optimizers.Adam(1e-3),loss=\"binary_crossentropy\") # since we predict between [0,1]\n",
    "\n",
    "deep_autoencoder.fit(X_train,X_train, # input and output are the same\n",
    "                       epochs=10,\n",
    "                       batch_size=128,\n",
    "                       shuffle=True,\n",
    "                       validation_data=(X_test,X_test),\n",
    "                       verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a91c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step\n"
     ]
    }
   ],
   "source": [
    "# reconstruction input using deep autoencoder\n",
    "X_test_deep_ae_reconstructed = deep_autoencoder.predict(X_test)\n",
    "\n",
    "# visualization function \n",
    "def plot_reconstructions(input,ae_output,n=10):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        ax = plt.subplot(3, n, i + 1)\n",
    "        plt.imshow(input[i].reshape(28, 28), cmap=\"gray\")\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Autoencoder\n",
    "        ax = plt.subplot(3, n, i + 1 + n)\n",
    "        plt.imshow(ae_output[i].reshape(28, 28), cmap=\"gray\")\n",
    "        plt.title(\"Deep AE\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_reconstructions(X_test,X_test_deep_ae_reconstructed,n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c720aa",
   "metadata": {},
   "source": [
    "This indicates that an image of a digit can be compressed into a 1x64 vector that is the latent representation of the digit space. Let's use a dimensionality reduction algorithm to plot these representations in 2D for each digit. We will use t-SNE for this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c3f47d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262us/step\n"
     ]
    }
   ],
   "source": [
    "# first we extract the trained encoder from the network\n",
    "encoder = tf.keras.Model(inputs=deep_autoencoder.input, outputs=encoded)\n",
    "\n",
    "# next we get the latent representation of the test images\n",
    "latent_vectors = encoder.predict(X_test)  # shape: (num_samples, 64)\n",
    "\n",
    "# now we run t-SNE on these vectors\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "latent_2d = tsne.fit_transform(latent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the representation in 2 dimensions\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=y_test, cmap='tab10', s=10)\n",
    "plt.colorbar(scatter, ticks=range(10))\n",
    "plt.title(\"t-SNE of Encoded MNIST Digits (64D → 2D)\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd32ea",
   "metadata": {},
   "source": [
    "The plot above shows that the deep autoencoder has learned to represent most digits separate from the others. That is, the latent vectors may have captured information like stroke width, edges, curves etc. to learn how digits differ from one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2d36c",
   "metadata": {},
   "source": [
    "#### Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c2b52",
   "metadata": {},
   "source": [
    "Let's now apply some noise to our inputs before being fed into the network to create a denoising autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_factor = 0.2\n",
    "\n",
    "# add noise to X_train\n",
    "X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
    "X_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)\n",
    "\n",
    "# clip noise to [0,1]\n",
    "X_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
    "X_test_noisy = np.clip(X_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca3d08",
   "metadata": {},
   "source": [
    "Repeat the deep autoencoder network but this time using noisy data as input and clean data as output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c34c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input\n",
    "input_img = tf.keras.layers.Input(shape=(784,))\n",
    "\n",
    "# encoder (linear)\n",
    "encoded = ...\n",
    "\n",
    "# decoder (linear)\n",
    "decoded = ...\n",
    "\n",
    "# the entire model\n",
    "denoising_autoencoder = ...\n",
    "\n",
    "denoising_autoencoder.compile(optimizer=tf.keras.optimizers.Adam(1e-3),loss=\"binary_crossentropy\") # since we predict between [0,1]\n",
    "\n",
    "denoising_autoencoder.fit(X_train_noisy,X_train, # input and output are the same\n",
    "                       epochs=10,\n",
    "                       batch_size=128,\n",
    "                       shuffle=True,\n",
    "                       validation_data=(X_test_noisy,X_test),\n",
    "                       verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39851de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_denoised = denoising_autoencoder.predict(X_test_noisy)\n",
    "\n",
    "# visualization function \n",
    "def plot_reconstructions(input,noisy_input,ae_output,n=10):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    for i in range(n):\n",
    "        # Noisy input\n",
    "        ax = plt.subplot(3, n, i + 1)\n",
    "        plt.imshow(noisy_input[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(\"Noisy\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Ground truth\n",
    "        ax = plt.subplot(3, n, i + 1 + n)\n",
    "        plt.imshow(input[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(\"Clean\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Denoised output\n",
    "        ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
    "        plt.imshow(ae_output[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(\"Denoised\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_reconstructions(X_test,X_test_noisy,X_test_denoised,n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0e7252",
   "metadata": {},
   "source": [
    "Let's also create a denoising convolutional autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0dfe0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# add channel information: (batch size, height, width, channels)\n",
    "X_train = np.expand_dims(X_train, axis=-1)  # (60000, 28, 28, 1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1138f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise\n",
    "noise_factor = 0.5\n",
    "X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
    "X_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)\n",
    "X_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
    "X_test_noisy = np.clip(X_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the denoising convolutional autoencoder\n",
    "input_img = tf.keras.layers.Input(shape=(28,28,1))\n",
    "\n",
    "# encoder\n",
    "\n",
    "\n",
    "# decoder\n",
    "\n",
    "\n",
    "denoising_c_autoencoder = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d94744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the network architecture\n",
    "denoising_c_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdfbf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "denoising_c_autoencoder.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='binary_crossentropy')\n",
    "\n",
    "# train model\n",
    "denoising_c_autoencoder.fit(\n",
    "    X_train_noisy, X_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test_noisy, X_test),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ea88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using autoencoder\n",
    "X_test_denoised = denoising_c_autoencoder.predict(X_test_noisy)\n",
    "\n",
    "# visualization function\n",
    "def plot_reconstructions(input,noisy_input,ae_output,n=10):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    for i in range(n):\n",
    "        # Noisy input\n",
    "        ax = plt.subplot(3, n, i + 1)\n",
    "        plt.imshow(noisy_input[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(\"Noisy\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Ground truth\n",
    "        ax = plt.subplot(3, n, i + 1 + n)\n",
    "        plt.imshow(input[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(\"Clean\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Denoised output\n",
    "        ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
    "        plt.imshow(ae_output[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(\"Denoised\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_reconstructions(X_test,X_test_noisy,X_test_denoised,n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df959b",
   "metadata": {},
   "source": [
    "In the above network we used UpSampling2D to upsample a feature map. Upsampling2D is purely deterministic (there is no learning or probabilities involved). It simply increases spatial dimensions by repeating or interpolating values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d608e",
   "metadata": {},
   "source": [
    "A Conv2DTranspose on the other hand also increases the spatial dimensions but does so by learning kernels. It works like a reverse of Conv2D by spreading the input into a larger output shape. That does mean there are more parameters to learn for the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9244d29b",
   "metadata": {},
   "source": [
    "For simple and quick upsampling, use UpSampling2D followed by a Conv2D layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ef6be",
   "metadata": {},
   "source": [
    "Let's checkout how Conv2dTranspose works with CIFAR-10 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed62098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "X_train = X_train.astype(\"float32\") / 255.\n",
    "X_test = X_test.astype(\"float32\") / 255.\n",
    "\n",
    "# define input dimensions\n",
    "input_img = tf.keras.layers.Input(shape=(32,32,3))\n",
    "\n",
    "# encoder\n",
    "\n",
    "# decoder\n",
    "\n",
    "autoencoder = ...\n",
    "\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='binary_crossentropy')\n",
    "\n",
    "# train model\n",
    "autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, X_test),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c52a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using network\n",
    "X_test_reconstructed = autoencoder.predict(X_test)\n",
    "\n",
    "# visualization function\n",
    "def plot_reconstructions(input, ae_output, n=10):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(input[i])\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Reconstructed\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(ae_output[i])\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_reconstructions(X_test, X_test_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def1cd6",
   "metadata": {},
   "source": [
    "Let's also take the latent representations, flatten them, and run them through t-SNE to see if we can find observable differences in the latent representation of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c543d0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# first extract the encoder\n",
    "encoder = tf.keras.Model(inputs=autoencoder.input, outputs=encoded)\n",
    "\n",
    "# then get the latent representations of the images\n",
    "encoded_imgs = encoder.predict(X_test)\n",
    "\n",
    "# next flatten the representations into 1x1024 dimensional vectors (4x4x128 = 1024)\n",
    "flattened = encoded_imgs.reshape(len(encoded_imgs), -1)  \n",
    "\n",
    "tsne = TSNE(n_components=3, perplexity=30)\n",
    "embeddings = tsne.fit_transform(flattened) # 3 dimensional embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f366f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar-10 labels\n",
    "cifar10_labels = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# 3D projection\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    embeddings[:, 0], embeddings[:, 1], embeddings[:, 2],\n",
    "    c=y_test.squeeze(), cmap='tab10', s=5\n",
    ")\n",
    "\n",
    "# set labels\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Dimension 3')\n",
    "ax.set_title(\"3D Visualization of Latent Space\")\n",
    "\n",
    "# create legend with class names\n",
    "cbar = plt.colorbar(scatter, ticks=range(10), pad=0.1)\n",
    "cbar.ax.set_yticklabels(cifar10_labels)\n",
    "cbar.set_label('Class')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419499c",
   "metadata": {},
   "source": [
    "Note that the goal of these networks is image reconstruction - not image classification. This means that even a deep autoencoder may not fully perform image classification simply because it has learned to compress the image into a latent diemnsion vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dff6e6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
