{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a53f06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# loading libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "from PIL import Image\n",
    "\n",
    "# import tensorflow and keras packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# let's also include different Models, Layers directly from keras\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM,Embedding,Input,GRU\n",
    "\n",
    "# use requests package to download some text\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb082231",
   "metadata": {},
   "source": [
    "This notebook details the steps to train an LSTM to predict the next **word** given some input. We will use a larger corpus of text (Pride and Prejudice). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c70daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to Pride and Prejudice in text form\n",
    "url = \"https://gutenberg.org/cache/epub/1342/pg1342.txt\"\n",
    "text = requests.get(url).text\n",
    "\n",
    "# clean text \n",
    "text = text[text.find(\"Chapter I.]\")+10:text.find(\"*** END OF THE PROJECT\")] # exclude metadata\n",
    "text = text.lower()\n",
    "print(f\"Length of text: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f095e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify unique words in text\n",
    "words = text.split()\n",
    "print(f\"Total words: {len(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39246e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the two dictionaries\n",
    "vocab = sorted(set(words))\n",
    "print(f\"Unique words: {len(vocab)}\")\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bca7d9",
   "metadata": {},
   "source": [
    "Now we can convert the entire text into a series of integers. Here, each word is represented by a unique integer ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e006e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([word2idx[w] for w in words], dtype=np.int32)\n",
    "print(\"First 20 encoded words:\", text_as_int[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b85c49",
   "metadata": {},
   "source": [
    "For this network, we will use a sequence length of 20 (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1aec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 20  # smaller since words carry more info\n",
    "examples_per_epoch = len(text_as_int) // (seq_length + 1)\n",
    "print(f\"Number of sequences: {examples_per_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d80434",
   "metadata": {},
   "source": [
    "Next, we will use tensorflow's from_tensor_slices function to create a stream of sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "018365c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = word_dataset.batch(seq_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716fdcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first words characters in the data\n",
    "for i, item in enumerate(word_dataset.take(10)):\n",
    "    print(item.numpy())\n",
    "\n",
    "# print the first sequence \n",
    "for i, item in enumerate(sequences.take(1)):\n",
    "    print(item.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbab27b",
   "metadata": {},
   "source": [
    "Next, we can define a function that creates our dataset of sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4805deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   input_text (first 20 chars)\n",
    "#   target_text (the next 20 chars, shifted by one position)\n",
    "def split_input_target(chunk):\n",
    "    input_seq = chunk[:-1]\n",
    "    target_seq = chunk[1:]\n",
    "    return input_seq, target_seq\n",
    "\n",
    "# apply the function to sequences\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba454a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input shape:\", input_example.shape)\n",
    "    print(\"Target shape:\", target_example.shape)\n",
    "    print(\"First input example (as IDs):\", input_example[0].numpy())\n",
    "    print(\"First target example (as IDs):\", target_example[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "22babd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4da59a",
   "metadata": {},
   "source": [
    "We have now created a dataset where each sequence is 20 words long and the target for that sequence is also 20 words long shifted by 1 word. We have also shuffled the input to the model to add some randomness. Note that buffer size if larger than the dataset size means an ideal situation for random selection. \n",
    "\n",
    "The Embedding layer will allow us to learn the relationship between characters. This is much better than one-hot encoding. So as part of predicting a sequence of characters, our model will also learn to better represent each character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7305b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters for the network\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(None,)),\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    LSTM(rnn_units, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    Dense(vocab_size)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "977bf6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 342ms/step - loss: 7.4948\n",
      "Epoch 2/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 344ms/step - loss: 6.7607\n",
      "Epoch 3/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 359ms/step - loss: 6.4968\n",
      "Epoch 4/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 403ms/step - loss: 6.2973\n",
      "Epoch 5/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 391ms/step - loss: 6.0838\n",
      "Epoch 6/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 381ms/step - loss: 5.8560\n",
      "Epoch 7/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 383ms/step - loss: 5.6671\n",
      "Epoch 8/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 384ms/step - loss: 5.5017\n",
      "Epoch 9/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 403ms/step - loss: 5.3525\n",
      "Epoch 10/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 386ms/step - loss: 5.2158\n",
      "Epoch 11/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 349ms/step - loss: 5.0854\n",
      "Epoch 12/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 339ms/step - loss: 4.9626\n",
      "Epoch 13/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 342ms/step - loss: 4.8450\n",
      "Epoch 14/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 346ms/step - loss: 4.7331\n",
      "Epoch 15/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 344ms/step - loss: 4.6242\n",
      "Epoch 16/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 342ms/step - loss: 4.5192\n",
      "Epoch 17/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 347ms/step - loss: 4.4167\n",
      "Epoch 18/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 356ms/step - loss: 4.3149\n",
      "Epoch 19/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 345ms/step - loss: 4.2163\n",
      "Epoch 20/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 342ms/step - loss: 4.1224\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(dataset, epochs=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "449236ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,293,184</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12864</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,599,232</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m3,293,184\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12864\u001b[0m)    │     \u001b[38;5;34m6,599,232\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,401,986</span> (131.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,401,986\u001b[0m (131.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,467,328</span> (43.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,467,328\u001b[0m (43.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,934,658</span> (87.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m22,934,658\u001b[0m (87.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39dae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if training takes too long, load pretrained model instead\n",
    "model = load_model(\"pride_lstm_word_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3a813387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_seq, num_generate=50, temperature=1.0):\n",
    "    # Tokenize the starting sequence into words\n",
    "    input_eval = [word2idx.get(w, 0) for w in start_seq.lower().split()]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    generated_words = []\n",
    "\n",
    "    for _ in range(num_generate):\n",
    "        predictions = model.predict(input_eval, verbose=0)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions[-1:], num_samples=1)[0, 0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        generated_words.append(idx2word[predicted_id])\n",
    "\n",
    "    return start_seq + ' ' + ' '.join(generated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f8cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, \"jane remained\", 10, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef73455",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model,\"he was\",5,5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model,\"it was\",5,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate_text(model, \"jane \", num_generate=1000, temperature=0.5)\n",
    "output = output.split(\".\")\n",
    "for sentence in output:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5df3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
