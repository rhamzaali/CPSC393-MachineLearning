{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53f06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# loading libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "from PIL import Image\n",
    "\n",
    "# import tensorflow and keras packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# let's also include different Models, Layers directly from keras\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM,Embedding,Input,GRU\n",
    "\n",
    "# use requests package to download some text\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb082231",
   "metadata": {},
   "source": [
    "This notebook details the steps to train an LSTM to predict the next **word** given some input. We will use a larger corpus of text (Pride and Prejudice). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c70daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 708325 characters\n"
     ]
    }
   ],
   "source": [
    "# url to Pride and Prejudice in text form\n",
    "url = \"https://gutenberg.org/cache/epub/1342/pg1342.txt\"\n",
    "text = requests.get(url).text\n",
    "\n",
    "# clean text \n",
    "text = text[text.find(\"Chapter I.]\")+10:text.find(\"*** END OF THE PROJECT\")] # exclude metadata\n",
    "text = text.lower()\n",
    "print(f\"Length of text: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f095e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 122410\n"
     ]
    }
   ],
   "source": [
    "# identify unique words in text\n",
    "words = text.split()\n",
    "print(f\"Total words: {len(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39246e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 12864\n"
     ]
    }
   ],
   "source": [
    "# generate the two dictionaries\n",
    "vocab = sorted(set(words))\n",
    "print(f\"Unique words: {len(vocab)}\")\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bca7d9",
   "metadata": {},
   "source": [
    "Now we can convert the entire text into a series of integers. Here, each word is represented by a unique integer ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e006e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 encoded words: [   34  6283  6274   218 11451 11657   345 11084   218 10253  6976  5856\n",
      "  8627  7860   218  5018  4727  7504  1250  5856]\n"
     ]
    }
   ],
   "source": [
    "text_as_int = np.array([word2idx[w] for w in words], dtype=np.int32)\n",
    "print(\"First 20 encoded words:\", text_as_int[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b85c49",
   "metadata": {},
   "source": [
    "For this network, we will use a sequence length of 20 (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f1aec83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 5829\n"
     ]
    }
   ],
   "source": [
    "seq_length = 20  # smaller since words carry more info\n",
    "examples_per_epoch = len(text_as_int) // (seq_length + 1)\n",
    "print(f\"Number of sequences: {examples_per_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d80434",
   "metadata": {},
   "source": [
    "Next, we will use tensorflow's from_tensor_slices function to create a stream of sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "018365c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = word_dataset.batch(seq_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716fdcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "6283\n",
      "6274\n",
      "218\n",
      "11451\n",
      "11657\n",
      "345\n",
      "11084\n",
      "218\n",
      "10253\n",
      "[   34  6283  6274   218 11451 11657   345 11084   218 10253  6976  5856\n",
      "  8627  7860   218  5018  4727  7504  1250  5856 11973]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 18:38:23.133730: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-11-05 18:38:23.138268: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# print the first words characters in the data\n",
    "for i, item in enumerate(word_dataset.take(10)):\n",
    "    print(item.numpy())\n",
    "\n",
    "# print the first sequence \n",
    "for i, item in enumerate(sequences.take(1)):\n",
    "    print(item.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbab27b",
   "metadata": {},
   "source": [
    "Next, we can define a function that creates our dataset of sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4805deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   input_text (first 20 chars)\n",
    "#   target_text (the next 20 chars, shifted by one position)\n",
    "def split_input_target(chunk):\n",
    "    input_seq = chunk[:-1]\n",
    "    target_seq = chunk[1:]\n",
    "    return input_seq, target_seq\n",
    "\n",
    "# apply the function to sequences\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba454a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (20,)\n",
      "Target shape: (20,)\n",
      "First input example (as IDs): 34\n",
      "First target example (as IDs): 6283\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input shape:\", input_example.shape)\n",
    "    print(\"Target shape:\", target_example.shape)\n",
    "    print(\"First input example (as IDs):\", input_example[0].numpy())\n",
    "    print(\"First target example (as IDs):\", target_example[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22babd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4da59a",
   "metadata": {},
   "source": [
    "We have now created a dataset where each sequence is 20 words long and the target for that sequence is also 20 words long shifted by 1 word. We have also shuffled the input to the model to add some randomness. Note that buffer size if larger than the dataset size means an ideal situation for random selection. \n",
    "\n",
    "The Embedding layer will allow us to learn the relationship between characters. This is much better than one-hot encoding. So as part of predicting a sequence of characters, our model will also learn to better represent each character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7305b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters for the network\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(None,)),\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    LSTM(rnn_units, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    Dense(vocab_size)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "977bf6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 342ms/step - loss: 7.4948\n",
      "Epoch 2/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 344ms/step - loss: 6.7607\n",
      "Epoch 3/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 359ms/step - loss: 6.4968\n",
      "Epoch 4/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 403ms/step - loss: 6.2973\n",
      "Epoch 5/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 391ms/step - loss: 6.0838\n",
      "Epoch 6/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 381ms/step - loss: 5.8560\n",
      "Epoch 7/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 383ms/step - loss: 5.6671\n",
      "Epoch 8/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 384ms/step - loss: 5.5017\n",
      "Epoch 9/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 403ms/step - loss: 5.3525\n",
      "Epoch 10/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 386ms/step - loss: 5.2158\n",
      "Epoch 11/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 349ms/step - loss: 5.0854\n",
      "Epoch 12/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 339ms/step - loss: 4.9626\n",
      "Epoch 13/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 342ms/step - loss: 4.8450\n",
      "Epoch 14/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 346ms/step - loss: 4.7331\n",
      "Epoch 15/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 344ms/step - loss: 4.6242\n",
      "Epoch 16/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 342ms/step - loss: 4.5192\n",
      "Epoch 17/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 347ms/step - loss: 4.4167\n",
      "Epoch 18/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 356ms/step - loss: 4.3149\n",
      "Epoch 19/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 345ms/step - loss: 4.2163\n",
      "Epoch 20/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 342ms/step - loss: 4.1224\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(dataset, epochs=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "449236ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,293,184</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12864</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,599,232</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m3,293,184\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12864\u001b[0m)    │     \u001b[38;5;34m6,599,232\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,401,986</span> (131.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,401,986\u001b[0m (131.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,467,328</span> (43.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,467,328\u001b[0m (43.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,934,658</span> (87.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m22,934,658\u001b[0m (87.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b39dae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if training takes too long, load pretrained model instead\n",
    "model = load_model(\"pride_lstm_word_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a813387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_seq, num_generate=50, temperature=1.0):\n",
    "    # Tokenize the starting sequence into words\n",
    "    input_eval = [word2idx.get(w, 0) for w in start_seq.lower().split()]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    generated_words = []\n",
    "\n",
    "    for _ in range(num_generate):\n",
    "        predictions = model.predict(input_eval, verbose=0)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions[-1:], num_samples=1)[0, 0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        generated_words.append(idx2word[predicted_id])\n",
    "\n",
    "    return start_seq + ' ' + ' '.join(generated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0f8cdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jane remained back her head silly warded _dined_ gracechurch cold child, everything'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, \"jane remained\", 10, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ef73455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he was shrink. becomes performers.” eye, here?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,\"he was\",5,5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4981be73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it was not be in the whole'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,\"it was\",5,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5f2ec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jane  had all the family could not the attentions to her, and she was of its writer were to be in the windows to be a book with the persuasion which she wanted to be a few minutes of the winter is not at the end so! existence instantly had been in the world in the paltry due to be down to be in her to miss darcy, she had been as if they were to be a day as to the inferiority of the invariable country costing her mother was i\n",
      "” business? should have been at his character for the country\n",
      "” they were to see the invariable mother and they were to be spoilt grave terms\n",
      "” london\n",
      "” retains the case\n",
      " chapter ii\n",
      " chapter spreading ill-judged the whole wish to be in the world to his character for the whole of the whole person and the whole of the day and about the attempt\n",
      "” “that is no more lucky\n",
      "” i am not help them; but the indulgence of the whole of their attentions of the good answered\n",
      " chapter “yes; repine; guidance of her to be so unwilling to be refused! banish of her mother’s schemes\n",
      " inconvenient gentlemen could not be down to speak of my father to do, pronouncing up she were to the whole of the young qualities are most agreeable to see the letter was very kind: and when she began to be in the least not be distressed, prey to be ordered her husband’s to confess that i have been all the whole of his children for the family may be in the next morning with great time to be at the room, unimportant, i confess that she had been in the whole of the very well as they were to be in the same the good names\n",
      " when the world to be to be as to the particulars, implied proud daughter! when he had been the very glad he had been a time, and a very many evenings in be in the party was not accuse him to be in the farm to be still of the hall, the next of the whole of the very impolitic eager to his father could not beyond: she was a single man to be in the want of your cheeks kindest you have been at the strength of the universities, he had been a servant, lost to what they were to be in the lakes, said to be on the whole of the alterations is not be a few minutes of the whole of the next of his own darcy, who had at the matter to be learnt, building, the very much of her own vice\n",
      " she had been as to speak of his friends, she had been to be in the whole of the misery of her mother’s _two_\n",
      " the particulars, develope\n",
      " and, after the regiment\n",
      " the whole of her mother and any man that he was at the drawing-room, “merely adapted to be in the whole party in the inferiority of the whole of the whole man to be down the good qualities are not to be a lieutenant’s sly, a certain that the dining-parlour\n",
      " eager to be in the _food_ of the particulars, copse, [illustration: moments, casual look\n",
      " “i am afraid that mr\n",
      " collins and i am sure she had been the day and elizabeth had been a few evenings to longbourn\n",
      " displeased, produced in a few days in the distinction should be in the whole of the whole of it, i am afraid it was some others, lift stretch of the coach; bonnet, by the ground, happy; descent tried, scolding how much to be in the coveys july, least\n",
      " sir?” authorize him to see him to be so well affected arrange assembled\n",
      " h\n",
      "t fitzwilliam, what she had been the contrary, she had been so much of his own\n",
      " of a well-known her sister, that he was not to the whole of his friend to be in the style of her sister, that the coach; falsehood, cried with him to be in the case\n",
      " “engaged justify mr\n",
      " darcy is the world to be equally disdained, and the indulgence of the spiteful ceremonious evenings to be a duchess; waiter is an evil\n",
      " dressing-room\n",
      " questions general\n",
      " did not the compliment so\n",
      " miss de purpose\n",
      " parade,” shield reverie\n",
      "” heart; and the wife to be a woman in the windows to be so exact; direction of the rest of last day in the most much of her daughter; hunting mean?” hand: i am very little to the end off\n",
      " i am glad it was a letter was a quarter path, lady catherine’s attentions to be so much to be in the dining-parlour\n",
      " things so replete into the day of his time to the whole of his coming to be in the whole of the offence and, and their attentions again\n",
      " “i am to be to be in the persuasion which he was not to you have been so much of her aunt to be not so much of the whole person to be very unwell this was beyond the man’s a woman of his own day the principal unfavourable time to see the room\n",
      " i am not be going to be hereafter her to be at the gentlemen gardiner’s only to see the perpetual elopement\n",
      " preceding might be so often! a few minutes of the lady,” were to be sure you to first\n",
      "” brother-in-law’s the day after the coach; tacit a few minutes of the whole party with the same, disgrace; dinner\n",
      " your ladyship’s forbearance; tenderly, will be a fresh place\n",
      " lady catherine’s blush of your affections and the country written and the good partners they? character\n",
      " i am glad you will be in the opportunity of happiness of it, and the rest\n",
      " i will not a few miles of the want of which the look in his cousin?” m^{r\n",
      "} ought,” bewitching not; constantly\n",
      "” names\n",
      " though she had been a few others, hint; sound\n",
      " you have been a that he had\n"
     ]
    }
   ],
   "source": [
    "output = generate_text(model, \"jane \", num_generate=1000, temperature=0.5)\n",
    "output = output.split(\".\")\n",
    "for sentence in output:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5df3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
