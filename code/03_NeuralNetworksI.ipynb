{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a53f06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# loading libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "\n",
    "# other helpers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import tensorflow and keras packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211783fc",
   "metadata": {},
   "source": [
    "Here's a single neuron with some input, weights, and a bias value. We can also calculate the weighted sum before the value is passed to an activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ee7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.8, 1.2])   # input features\n",
    "w = np.array([0.6, -0.1])  # weights\n",
    "b = 0.1                    # bias\n",
    "z = np.dot(w, x) + b\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ac5e8",
   "metadata": {},
   "source": [
    "Activation functions create a non-linear output (excluding step and linear activation) for which a gradient can be calculated. \n",
    "Here, we will plot sigmoid, tanh, and ReLU activation function output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccec7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "x_values = ...\n",
    "\n",
    "df_activations = pd.DataFrame({\n",
    "    'x':x_values,\n",
    "    'sigmoid' : sigmoid(x_values),\n",
    "    'tanh' : tanh(x_values),\n",
    "    'relu' : relu(x_values) \n",
    "})\n",
    "\n",
    "(\n",
    "    ggplot(df_activations)+\n",
    "    geom_line(aes(x='x', y='sigmoid'), color='blue')+\n",
    "    geom_line(aes(x='x', y='tanh'), color='orange')+\n",
    "    geom_line(aes(x='x', y='relu'), color='green')+\n",
    "    labs(title='Activation Functions', x='Input', y='Activation')+\n",
    "    theme_minimal()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc5ce1a",
   "metadata": {},
   "source": [
    "Let's train a linear regression model using the building blocks of a neural network implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5db336",
   "metadata": {},
   "source": [
    "When building a neural network, think of the following key items:\n",
    "- The structure of your model (how many nodes/layers? which activation function to use?)\n",
    "- The loss function of your model\n",
    "- The optimizer you will use to train the model (more to come on this)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d9ebc8",
   "metadata": {},
   "source": [
    "The input and output layers are determined by your feature space (size and type of input) and the target (what form of output you expect). For a regression problem, your output layer will have a single node. For a multi-class classification, it will be n nodes representing n classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18882c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we load our data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/cristobalvch/Spotify-Machine-Learning/refs/heads/master/data/data_moods.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e94b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define our feature space and target\n",
    "# we will predict 'energy' using the other features\n",
    "\n",
    "features = ['danceability','popularity','length','acousticness','energy','instrumentalness','liveness','valence','loudness','speechiness','tempo']\n",
    "\n",
    "X = df[features]\n",
    "y = df['energy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1093f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also standardize our feature space\n",
    "\n",
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c6a89",
   "metadata": {},
   "source": [
    "### Building blocks of a neural network in Tensorflow/Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad17d4f",
   "metadata": {},
   "source": [
    "#### Model Structure\n",
    "\n",
    "| **Component**   | **Function / Parameter**                                | **Description**                                              |\n",
    "| --------------- | ------------------------------------------------------- | ------------------------------------------------------------ |\n",
    "| **Model**       | `tf.keras.Sequential()` or subclass of `tf.keras.Model` | Container that defines the model architecture.               |\n",
    "| **Layers**      | `tf.keras.layers.Dense(units, activation)`              | Fully connected layers used to build the model.              |\n",
    "| **Units**       | `units=...`                                             | Number of neurons in a layer.                                |\n",
    "| **Activation**  | `activation='relu'`, `'sigmoid'`, `'softmax'`           | Function applied to layer output to introduce non-linearity. |\n",
    "| **Input Shape** | `input_shape=(...)`                                     | Shape of input data (required in the first layer).           |\n",
    "\n",
    "#### Compiling\n",
    "\n",
    "| **Component**     | **Function / Parameter**                    | **Description**                                                          |\n",
    "| ----------------- | ------------------------------------------- | ------------------------------------------------------------------------ |\n",
    "| **Compile**       | `model.compile()`                           | Prepares the model for training by setting loss, optimizer, and metrics. |\n",
    "| **Loss Function** | `loss='mse'`, `'binary_crossentropy'`, etc. | Measures the difference between predictions and actual values.           |\n",
    "| **Optimizer**     | `optimizer='sgd'`, `'adam'`, etc.           | Algorithm to update weights based on gradients.                          |\n",
    "| **Metrics**       | `metrics=['accuracy']`, etc.                | List of metrics to evaluate during training/testing.                     |\n",
    "\n",
    "#### Training and Evaluation\n",
    "\n",
    "| **Component**  | **Function / Parameter**         | **Description**                                         |\n",
    "| -------------- | -------------------------------- | ------------------------------------------------------- |\n",
    "| **Fit**        | `model.fit(X, y, epochs=...)`    | Trains the model using input data and labels.           |\n",
    "| **Epochs**     | `epochs=...`                     | Number of complete passes through the training data.    |\n",
    "| **Batch Size** | `batch_size=...`                 | Number of samples processed before model weight update. |\n",
    "| **Evaluate**   | `model.evaluate(X_test, y_test)` | Tests model performance on new data.                    |\n",
    "| **Predict**    | `model.predict(X_new)`           | Uses the model to make predictions on new data.         |\n",
    "\n",
    "#### Other Blocks\n",
    "\n",
    "| **Component**     | **Function / Parameter**                     | **Description**                                                         |\n",
    "| ----------------- | -------------------------------------------- | ----------------------------------------------------------------------- |\n",
    "| **Model Weights** | `model.get_weights()`, `model.set_weights()` | Access or modify model parameters directly.                             |\n",
    "| **Callbacks**     | `callbacks=[...]`                            | Tools like early stopping, learning rate scheduling, and checkpointing. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facef92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets build a simple neural network: one neural, no activation function\n",
    "model = tf.keras.Sequential([...])\n",
    "\n",
    "# compile the model\n",
    "\n",
    "# train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b84c6",
   "metadata": {},
   "source": [
    "During each epoch:  (actually done in batches but we will discuss this later)\n",
    "- 1. Forward Pass: model makes prediction on X_train observations\n",
    "- 2. Loss Computation: MSE is calculated between predicted and true values\n",
    "- 3. Backpropagation: gradients are computed with respect to weights\n",
    "- 4. SDG Optimizer: updates weights\n",
    "- 5. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6eb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print learned weights and bias\n",
    "weights, bias = ...\n",
    "print(\"Weights:\", weights.flatten())\n",
    "print(\"Bias:\", bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Mean Squared Error on training set:\", mse)\n",
    "\n",
    "mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Mean Squared Error on test set:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15190fc",
   "metadata": {},
   "source": [
    "Let's build a deep neural network - that means adding some hidden layers! Recall that if no activation function is used then all layers combine into a single linear combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eead35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a deep neural network with multiple layers and activation functions\n",
    "\n",
    "deep_model = tf.keras.Sequential([...])\n",
    "\n",
    "# compile the model\n",
    "\n",
    "# train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12479a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate deep_model\n",
    "mse = deep_model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Mean Squared Error on training set:\", mse)\n",
    "\n",
    "mse = deep_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Mean Squared Error on test set:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "689b1d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the model summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
