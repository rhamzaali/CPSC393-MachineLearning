{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a53f06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# loading libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "\n",
    "# import tensorflow and keras packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9395c0d3",
   "metadata": {},
   "source": [
    "We will load the MNIST data again. This time, we will change the labels to be in a more standardized form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5efc79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data from keras.datasets\n",
    "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train_mnist = X_train_mnist.reshape(-1, 28*28).astype('float32') / 255.0\n",
    "X_test_mnist = X_test_mnist.reshape(-1, 28*28).astype('float32') / 255.0\n",
    "\n",
    "# Convert y labels to one-hot encoded vectors\n",
    "y_train_mnist = keras.utils.to_categorical(y_train_mnist, num_classes=10)\n",
    "y_test_mnist = keras.utils.to_categorical(y_test_mnist, num_classes=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b862968c",
   "metadata": {},
   "source": [
    "Regularization techniques ensure that a deep neural network is generalized - avoids overfitting in particular. Some techniques we can employ:\n",
    "- Penalization\n",
    "- Dropout\n",
    "- Batch Normalization\n",
    "- Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07aadb1",
   "metadata": {},
   "source": [
    "Let's first build a deep neural network to classify digits without any regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a86e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ...\n",
    "\n",
    "model.compile(...)\n",
    "\n",
    "history_no_reg = model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ada20c",
   "metadata": {},
   "source": [
    "Let's introduce L2 weight regularization. This is theoretically a weight decay which will ensure that weights remain small and not one neuron influences inferences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35484ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ...\n",
    "\n",
    "model.compile(...)\n",
    "\n",
    "history_l2_reg = model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760209e7",
   "metadata": {},
   "source": [
    "Let's also introduce Dropout. Dropout with a probability p will randomly drop neurons from network during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ...\n",
    "\n",
    "model.compile(...)\n",
    "\n",
    "history_l2_drop_reg = model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50765d69",
   "metadata": {},
   "source": [
    "Now, we can compare the models to see if any generalization is observed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2605b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_history = {\"None\":history_no_reg.history,\n",
    "                 \"L2\":history_l2_reg.history,\n",
    "                 \"L2/Dropout\":history_l2_drop_reg.history}\n",
    "\n",
    "# Loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "for name in model_history:\n",
    "    plt.plot(model_history[name]['loss'], label=f'{name}')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Training Accuracy\n",
    "plt.figure(figsize=(10, 4))\n",
    "for name in model_history:\n",
    "    plt.plot(model_history[name]['accuracy'], label=f'{name}')\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "plt.figure(figsize=(10, 4))\n",
    "for name in model_history:\n",
    "    plt.plot(model_history[name]['val_accuracy'], label=f'{name}')\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b40c97",
   "metadata": {},
   "source": [
    "The plots show that training and validation accuracies are closer after model training. Note that with regularization, the model has not reached convergence and could use more training steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb0b95",
   "metadata": {},
   "source": [
    "Another option would be to use Early Stopping. This mechanism stops training when validation loss stops improving, preventing overfitting. We can use the EarlyStopping callback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbfcaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "        keras.layers.Dense(256,activation='relu',input_shape=(784,)),\n",
    "        keras.layers.Dense(256,activation='relu'),\n",
    "        keras.layers.Dense(10,activation='softmax') # output layer\n",
    "    ])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\", #keras.optimizers.SGD(0.01)\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = ...\n",
    "\n",
    "history_no_reg = model.fit(X_train_mnist,y_train_mnist,\n",
    "                           epochs=20,verbose=1,batch_size=128,\n",
    "                           validation_data=(X_test_mnist,y_test_mnist),\n",
    "                           callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f53b6",
   "metadata": {},
   "source": [
    "Our last regularization step (not include data augmentation), is Batch Normalization. This normalizes activations of each layer to mean = 0 and std = 1, per mini-batch. It stabilizes and speeds up training. Note that we can apply Batch Normalization before the activation function is applied. That is, normalize the weight sum and then apply an activation function on top. You can apply Batch Normalization after activation function as well, but be careful as ReLU can result in a lot of 0s.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8637d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ...\n",
    "\n",
    "model.compile(...)\n",
    "\n",
    "history_no_reg = model.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aafe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch normalization adds additional parameters for the networks to learn\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7cb1e",
   "metadata": {},
   "source": [
    "The network now has to learn two new parameters (scale and shift) per node, plus the running running mean and variance but these only need to be tracked not learned. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802a332",
   "metadata": {},
   "source": [
    "Traditionally, the layout for a feed forward neural network should be\n",
    "1. Define Input Layer\n",
    "2. Add N Hidden Layers - for each, add L2 regularization, Activation Function, Batch Normalization (before/after activation)\n",
    "3. Add Dropout between layers (after activation)\n",
    "4. Define Output Layer with task-depended activation function (softmax for classification, sigmoid for binary, none for regression/linear)\n",
    "5. Define Optimizer, Loss Function\n",
    "6. Define Early Stopping parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
