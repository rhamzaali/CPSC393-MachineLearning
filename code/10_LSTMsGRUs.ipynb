{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a53f06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# loading libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "from PIL import Image\n",
    "\n",
    "# import tensorflow and keras packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# let's also include different Models, Layers directly from keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM,Embedding,Input,GRU\n",
    "\n",
    "# use requests package to download some text\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb082231",
   "metadata": {},
   "source": [
    "Let's train a sequential model using the text from Alice in Wonderland. Project Gutenberg website hosts text versions of many classics. Feel free to donwload another if you want to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30c70daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 166223 characters\n"
     ]
    }
   ],
   "source": [
    "# url to Alice in Wonderland in text form\n",
    "url = \"https://gutenberg.org/cache/epub/11/pg11.txt\"\n",
    "text = requests.get(url).text\n",
    "\n",
    "text = text[1451:] # exclude metadata\n",
    "print(f\"Length of text: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f095e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER I.\n",
      "Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into\n",
      "the book her sister was reading, but it had no pictures or\n",
      "conversations in it, “and what is the use of a book,” thought Alice\n",
      "“without pictures or conversations?”\n",
      "\n",
      "So she was considering in her own mind (as well as she could, for the\n",
      "hot day made her feel very sleepy and stupid), whether the pleasure of\n",
      "making a \n"
     ]
    }
   ],
   "source": [
    "# let's print the first 500 characters from text \n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512958d9",
   "metadata": {},
   "source": [
    "Models do not understand text like we do. We will need to create some mapping from text to integers to then pass that along to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39246e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters in the text: ['\\n', '\\r', ' ', '!', '$', '%', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ù', '—', '‘', '’', '“', '”', '•', '™'] \n",
      "\n",
      "char2idx {'\\n': 0, '\\r': 1, ' ': 2, '!': 3, '$': 4, '%': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '?': 26, '[': 27, ']': 28, '_': 29, 'a': 30, 'b': 31, 'c': 32, 'd': 33, 'e': 34, 'f': 35, 'g': 36, 'h': 37, 'i': 38, 'j': 39, 'k': 40, 'l': 41, 'm': 42, 'n': 43, 'o': 44, 'p': 45, 'q': 46, 'r': 47, 's': 48, 't': 49, 'u': 50, 'v': 51, 'w': 52, 'x': 53, 'y': 54, 'z': 55, 'ù': 56, '—': 57, '‘': 58, '’': 59, '“': 60, '”': 61, '•': 62, '™': 63} \n",
      "\n",
      "idx2char {0: '\\n', 1: '\\r', 2: ' ', 3: '!', 4: '$', 5: '%', 6: \"'\", 7: '(', 8: ')', 9: '*', 10: ',', 11: '-', 12: '.', 13: '/', 14: '0', 15: '1', 16: '2', 17: '3', 18: '4', 19: '5', 20: '6', 21: '7', 22: '8', 23: '9', 24: ':', 25: ';', 26: '?', 27: '[', 28: ']', 29: '_', 30: 'a', 31: 'b', 32: 'c', 33: 'd', 34: 'e', 35: 'f', 36: 'g', 37: 'h', 38: 'i', 39: 'j', 40: 'k', 41: 'l', 42: 'm', 43: 'n', 44: 'o', 45: 'p', 46: 'q', 47: 'r', 48: 's', 49: 't', 50: 'u', 51: 'v', 52: 'w', 53: 'x', 54: 'y', 55: 'z', 56: 'ù', 57: '—', 58: '‘', 59: '’', 60: '“', 61: '”', 62: '•', 63: '™'}\n"
     ]
    }
   ],
   "source": [
    "# lowercase the text\n",
    "text = text.lower()\n",
    "\n",
    "# create a list of all characters in the text\n",
    "chars = sorted(set(text))\n",
    "\n",
    "print(\"Characters in the text:\",chars,\"\\n\")\n",
    "\n",
    "# we will create two look up dictionaries \n",
    "## char2idx: maps each character to a unique integer (ID)\n",
    "## idx2char: maps integer IDs back to characters\n",
    "char2idx = {c: i for i, c in enumerate(chars)}\n",
    "idx2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "print(\"char2idx\",char2idx,\"\\n\")\n",
    "print(\"idx2char\",idx2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bca7d9",
   "metadata": {},
   "source": [
    "Now we can convert the entire text into a series of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8e006e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters in the text: 166223\n",
      "[32 37 30 45 49 34 47  2 38 12  1  0 33 44 52 43  2 49 37 34  2 47 30 31\n",
      " 31 38 49 11 37 44 41 34  1  0  1  0  1  0 30 41 38 32 34  2 52 30 48  2\n",
      " 31 34 36 38 43 43 38 43 36  2 49 44  2 36 34 49  2 51 34 47 54  2 49 38\n",
      " 47 34 33  2 44 35  2 48 38 49 49 38 43 36  2 31 54  2 37 34 47  2 48 38\n",
      " 48 49 34 47  2 44 43  2 49 37 34  1  0 31 30 43 40 10  2 30 43 33  2 44\n",
      " 35  2 37 30 51 38 43 36  2 43 44 49 37 38 43 36  2 49 44  2 33 44 24  2\n",
      " 44 43 32 34  2 44 47  2 49 52 38 32 34  2 48 37 34  2 37 30 33  2 45 34\n",
      " 34 45 34 33  2 38 43 49 44  1  0 49 37 34  2 31 44 44 40  2 37 34 47  2\n",
      " 48 38 48 49 34 47  2 52 30 48  2 47 34 30 33 38 43 36 10  2 31 50 49  2\n",
      " 38 49  2 37 30 33  2 43 44  2 45 38 32 49 50 47 34 48  2 44 47  1  0 32\n",
      " 44 43 51 34 47 48 30 49 38 44 43 48  2 38 43  2 38 49 10  2 60 30 43 33\n",
      "  2 52 37 30 49  2 38 48  2 49 37 34  2 50 48 34  2 44 35  2 30  2 31 44\n",
      " 44 40 10 61  2 49 37 44 50 36 37 49  2 30 41 38 32 34  1  0 60 52 38 49\n",
      " 37 44 50 49  2 45 38 32 49 50 47 34 48  2 44 47  2 32 44 43 51 34 47 48\n",
      " 30 49 38 44 43 48 26 61  1  0  1  0 48 44  2 48 37 34  2 52 30 48  2 32\n",
      " 44 43 48 38 33 34 47 38 43 36  2 38 43  2 37 34 47  2 44 52 43  2 42 38\n",
      " 43 33  2  7 30 48  2 52 34 41 41  2 30 48  2 48 37 34  2 32 44 50 41 33\n",
      " 10  2 35 44 47  2 49 37 34  1  0 37 44 49  2 33 30 54  2 42 30 33 34  2\n",
      " 37 34 47  2 35 34 34 41  2 51 34 47 54  2 48 41 34 34 45 54  2 30 43 33\n",
      "  2 48 49 50 45 38 33  8 10  2 52 37 34 49 37 34 47  2 49 37 34  2 45 41\n",
      " 34 30 48 50 47 34  2 44 35  1  0 42 30 40 38 43 36  2 30  2]\n"
     ]
    }
   ],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text], dtype=np.int32)\n",
    "\n",
    "print(\"Characters in the text:\",len(text_as_int))\n",
    "print(text_as_int[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b85c49",
   "metadata": {},
   "source": [
    "Now let's define the input for the model. This model will predict the **next character** given an input and not the next word. We will create our inputs to be 100 characters long. Think of this as a time window with 100 steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f1aec83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1645 sequences in the input\n"
     ]
    }
   ],
   "source": [
    "# sequence length\n",
    "seq_length = 100\n",
    "\n",
    "# use sequence length to calculate number of sequences we can produce\n",
    "examples_per_epoch = len(text_as_int) // (seq_length + 1)\n",
    "\n",
    "print(examples_per_epoch,\"sequences in the input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d80434",
   "metadata": {},
   "source": [
    "Next, we will use tensorflow's from_tensor_slices function to create a stream of sequences. \n",
    "\n",
    "\n",
    "Text: \"alice in wonderland\"\n",
    "\n",
    "\n",
    "↓\n",
    "\n",
    "\n",
    "Integer IDs: [1, 12, 9, 3, 5, ...]\n",
    "\n",
    "\n",
    "↓\n",
    "\n",
    "\n",
    "Dataset from tensor slices:\n",
    "\n",
    "\n",
    "  [1] → [12] → [9] → [3] → [5] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "018365c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset where each element is a single character (integer-version)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# a sequence is then a group of these characters \n",
    "# + 1 so that we take 100 characters as input and predict the character shifted by 1\n",
    "# drop_remainder to drop a sequence if it's length is below our desired length\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "716fdcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "37\n",
      "30\n",
      "45\n",
      "49\n",
      "34\n",
      "47\n",
      "2\n",
      "38\n",
      "12\n",
      "[32 37 30 45 49 34 47  2 38 12  1  0 33 44 52 43  2 49 37 34  2 47 30 31\n",
      " 31 38 49 11 37 44 41 34  1  0  1  0  1  0 30 41 38 32 34  2 52 30 48  2\n",
      " 31 34 36 38 43 43 38 43 36  2 49 44  2 36 34 49  2 51 34 47 54  2 49 38\n",
      " 47 34 33  2 44 35  2 48 38 49 49 38 43 36  2 31 54  2 37 34 47  2 48 38\n",
      " 48 49 34 47  2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 18:00:51.435847: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-11-05 18:00:51.440916: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# print the first 10 characters in the data\n",
    "for i, item in enumerate(char_dataset.take(10)):\n",
    "    print(item.numpy())\n",
    "\n",
    "# print the first sequence \n",
    "for i, item in enumerate(sequences.take(1)):\n",
    "    print(item.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbab27b",
   "metadata": {},
   "source": [
    "Next, we can define a function that creates our dataset of sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4805deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   input_text (first 100 chars)\n",
    "#   target_text (the next 100 chars, shifted by one position)\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# apply the function to sequences\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba454a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (100,)\n",
      "Target shape: (100,)\n",
      "First input example (as IDs): 32\n",
      "First target example (as IDs): 37\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input shape:\", input_example.shape)\n",
    "    print(\"Target shape:\", target_example.shape)\n",
    "    print(\"First input example (as IDs):\", input_example[0].numpy())\n",
    "    print(\"First target example (as IDs):\", target_example[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22babd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # how many sequences the model sees at once\n",
    "BUFFER_SIZE = 10000 # controls randomness of shuffle\n",
    "\n",
    "# shuffle randomly picks elements from a buffer of size 10000 - large buffer = more random shuffling\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4da59a",
   "metadata": {},
   "source": [
    "We have now created a dataset where each sequence is 100 characters long and the target for that sequence is also 100 characters long shifted by 1 character. We have also shuffled the input to the model to add some randomness. Note that buffer size if larger than the dataset size means an ideal situation for random selection. \n",
    "\n",
    "Let's also define the parameters for our network. While converting characters into integers was the first step, these integer IDs are arbitrary and do not truly represent characters and how they should be represented. The Embedding layer will allow us to learn the relationship between characters. This is much better than one-hot encoding. So as part of predicting a sequence of characters, our model will also learn to better represent each character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7305b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters for the network\n",
    "vocab_size = len(chars)   # number of unique characters\n",
    "embedding_dim = 256       # dimensions of character embeddings\n",
    "rnn_units = 512           # LSTM hidden units\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(None,)), # None makes the model general to different sizes of inputs\n",
    "    Embedding(vocab_size, embedding_dim), # add an Embedding layer to convert integer \n",
    "                                          #representation of characters into vector representation\n",
    "    LSTM(rnn_units, return_sequences=True), # return output at each time step\n",
    "    Dropout(0.2),\n",
    "    Dense(vocab_size) # output is a probability distribution across all characters\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True) # sparse categories so apply softmax to get probability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "977bf6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 256ms/step - loss: 3.3622\n",
      "Epoch 2/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 287ms/step - loss: 2.7906\n",
      "Epoch 3/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 291ms/step - loss: 2.4903\n",
      "Epoch 4/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 286ms/step - loss: 2.3512\n",
      "Epoch 5/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 282ms/step - loss: 2.2586\n",
      "Epoch 6/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 301ms/step - loss: 2.1713\n",
      "Epoch 7/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 286ms/step - loss: 2.0918\n",
      "Epoch 8/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 271ms/step - loss: 2.0192\n",
      "Epoch 9/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 279ms/step - loss: 1.9526\n",
      "Epoch 10/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 304ms/step - loss: 1.8910\n",
      "Epoch 11/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 282ms/step - loss: 1.8375\n",
      "Epoch 12/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 302ms/step - loss: 1.7884\n",
      "Epoch 13/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 317ms/step - loss: 1.7460\n",
      "Epoch 14/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 288ms/step - loss: 1.7047\n",
      "Epoch 15/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 295ms/step - loss: 1.6646\n",
      "Epoch 16/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 293ms/step - loss: 1.6324\n",
      "Epoch 17/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 284ms/step - loss: 1.5974\n",
      "Epoch 18/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 301ms/step - loss: 1.5685\n",
      "Epoch 19/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 319ms/step - loss: 1.5396\n",
      "Epoch 20/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 307ms/step - loss: 1.5134\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(dataset, epochs=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "449236ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │        \u001b[38;5;34m16,384\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m32,832\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,872,386</span> (18.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,872,386\u001b[0m (18.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,624,128</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,624,128\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,248,258</span> (12.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,248,258\u001b[0m (12.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6958b9f",
   "metadata": {},
   "source": [
    "Once our network is trained, we can now make predictions from it. We will define a function that takes in a starting sequence and then predicts what the next character should be. That then becomes the input again and we can keep predicting the next character to build sentences. \n",
    "\n",
    "Temperature regulates how conservative or random the prediction should be. Predictions are the raw predictions from the model which are to passed to a softmax function to calculate probabilities. By dividing the logits with temperature, we can change the shape of the probability distribution. \n",
    "- T = 1.0: no change \n",
    "- T < 1.0: model is more predictable/confident - largest logits become more prominent\n",
    "- T > 1.0: model is more random/creative - flattens distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a813387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,starting_seq,num_generate=1000,temperature=1.0):\n",
    "    input_eval = [char2idx[c] for c in starting_seq.lower()] # convert input chars to ints\n",
    "    input_eval = tf.expand_dims(input_eval, 0)  # add batch dimension for tf\n",
    "\n",
    "    generated_text = [] # here we will store the predicted characters\n",
    "\n",
    "    for i in range(num_generate):\n",
    "        predictions = model.predict(input_eval, verbose=0)\n",
    "        predictions = tf.squeeze(predictions, 0) # remove batch dimension\n",
    "\n",
    "        # apply temperature\n",
    "        predictions = predictions / temperature\n",
    "\n",
    "        # get the predicted character for each time step\n",
    "        # but we only need the very last predicted character (-1)\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        # now this predicted character becomes the new input to make the next prediction\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        generated_text.append(idx2char[predicted_id]) # add prediction to list\n",
    "    \n",
    "    return starting_seq + ''.join(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0f8cdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alice g!” w'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,\"alice \",5,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ef73455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alice i‘:24'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,\"alice \",5,5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4981be73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alice wan t'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,\"alice \",5,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5f2ec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice finuthe 14™ the the oungald ing bre whed tin tllexlomathan but atalated we the t withanon at s wand the te aling t futhery bupe it ate anout linge any, aleralid was sent sthe athe d than thery oure tidinere d winth arash wred id ange.\n",
      "t the thed te wanore then are ung s t be ale allloulout the.\n",
      "we t arere se thanorere s he o the one are orealll by sted allous—\n",
      "s t st thed t t athed se all the ay t in fon sed te t agoroury in atorind s pe airupoure sthe thed ano e inuthed thed t t touse at s at t tole ine st sinore is foushe ingonesery ti’thered th therede th alarory teren wint  shene  outere forithe he bupe t they se t “thed arede an st sed mery anag t ted che ale ang he walit at y t anthereany t ng the thesthe angrme the tling the at the thes ly s thonand angore t it ath thely te the whe at ond wand ous at f y ite and t inoule owincan tuse inorat she ore t pedre tlide the we t the anf h, t at on be te he ale onot s wan won t the ang t se bbouthe sare anonkin t s by or allory the the\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"alice \", num_generate=1000, temperature=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4584e6b",
   "metadata": {},
   "source": [
    "Let's now train a GRU using the same setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters for the network\n",
    "vocab_size = len(chars)   # number of unique characters\n",
    "embedding_dim = 256       # dimensions of character embeddings\n",
    "rnn_units = 512           # LSTM hidden units\n",
    "\n",
    "# the dropout here is within the GRU layer call\n",
    "# dropout will randomly dropinput features at each time step\n",
    "# recurrent_dropout will randomly drop hidden states from being passed to the next one\n",
    "model = Sequential([\n",
    "    Input(shape=(None,)), # None makes the model general to different sizes of inputs\n",
    "    Embedding(vocab_size, embedding_dim), # add an Embedding layer to convert integer \n",
    "                                          #representation of characters into vector representation\n",
    "    GRU(rnn_units, return_sequences=True,dropout=0.3,recurrent_dropout=0.3), \n",
    "    Dense(vocab_size) # output is a probability distribution across all characters\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True) # sparse categories so apply softmax to get probability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6324b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "history = model.fit(dataset, epochs=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6da48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4552e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model,\"alice \",5,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model,\"alice \",5,5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04611e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model,\"alice \",5,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, \"alice \", num_generate=1000, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ccb1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
