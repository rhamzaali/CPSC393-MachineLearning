{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53f06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# loading libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "from PIL import Image\n",
    "\n",
    "# import tensorflow and keras packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# let's also include different Models, Layers directly from keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM,Embedding,Input,GRU\n",
    "\n",
    "# use requests package to download some text\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb082231",
   "metadata": {},
   "source": [
    "Let's train a sequential model using the text from Alice in Wonderland. Project Gutenberg website hosts text versions of many classics. Feel free to donwload another if you want to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c70daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to Alice in Wonderland in text form\n",
    "url = \"https://gutenberg.org/cache/epub/11/pg11.txt\"\n",
    "text = requests.get(url).text\n",
    "\n",
    "text = text[1451:] # exclude metadata\n",
    "print(f\"Length of text: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f095e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's print the first 500 characters from text \n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512958d9",
   "metadata": {},
   "source": [
    "Models do not understand text like we do. We will need to create some mapping from text to integers to then pass that along to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39246e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase the text\n",
    "text = text.lower()\n",
    "\n",
    "# create a list of all characters in the text\n",
    "chars = sorted(set(text))\n",
    "\n",
    "print(\"Characters in the text:\",chars,\"\\n\")\n",
    "\n",
    "# we will create two look up dictionaries \n",
    "## char2idx: maps each character to a unique integer (ID)\n",
    "## idx2char: maps integer IDs back to characters\n",
    "char2idx = {c: i for i, c in enumerate(chars)}\n",
    "idx2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "print(\"char2idx\",char2idx,\"\\n\")\n",
    "print(\"idx2char\",idx2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bca7d9",
   "metadata": {},
   "source": [
    "Now we can convert the entire text into a series of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e006e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text], dtype=np.int32)\n",
    "\n",
    "print(\"Characters in the text:\",len(text_as_int))\n",
    "print(text_as_int[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b85c49",
   "metadata": {},
   "source": [
    "Now let's define the input for the model. This model will predict the **next character** given an input and not the next word. We will create our inputs to be 100 characters long. Think of this as a time window with 100 steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1aec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length\n",
    "seq_length = 100\n",
    "\n",
    "# use sequence length to calculate number of sequences we can produce\n",
    "examples_per_epoch = len(text_as_int) // (seq_length + 1)\n",
    "\n",
    "print(examples_per_epoch,\"sequences in the input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d80434",
   "metadata": {},
   "source": [
    "Next, we will use tensorflow's from_tensor_slices function to create a stream of sequences. \n",
    "\n",
    "\n",
    "Text: \"alice in wonderland\"\n",
    "\n",
    "\n",
    "↓\n",
    "\n",
    "\n",
    "Integer IDs: [1, 12, 9, 3, 5, ...]\n",
    "\n",
    "\n",
    "↓\n",
    "\n",
    "\n",
    "Dataset from tensor slices:\n",
    "\n",
    "\n",
    "  [1] → [12] → [9] → [3] → [5] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "018365c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset where each element is a single character (integer-version)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# a sequence is then a group of these characters \n",
    "# + 1 so that we take 100 characters as input and predict the character shifted by 1\n",
    "# drop_remainder to drop a sequence if it's length is below our desired length\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716fdcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 characters in the data\n",
    "for i, item in enumerate(char_dataset.take(10)):\n",
    "    print(item.numpy())\n",
    "\n",
    "# print the first sequence \n",
    "for i, item in enumerate(sequences.take(1)):\n",
    "    print(item.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbab27b",
   "metadata": {},
   "source": [
    "Next, we can define a function that creates our dataset of sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c4805deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   input_text (first 100 chars)\n",
    "#   target_text (the next 100 chars, shifted by one position)\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# apply the function to sequences\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba454a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input shape:\", input_example.shape)\n",
    "    print(\"Target shape:\", target_example.shape)\n",
    "    print(\"First input example (as IDs):\", input_example[0].numpy())\n",
    "    print(\"First target example (as IDs):\", target_example[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "22babd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # how many sequences the model sees at once\n",
    "BUFFER_SIZE = 10000 # controls randomness of shuffle\n",
    "\n",
    "# shuffle randomly picks elements from a buffer of size 10000 - large buffer = more random shuffling\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4da59a",
   "metadata": {},
   "source": [
    "We have now created a dataset where each sequence is 100 characters long and the target for that sequence is also 100 characters long shifted by 1 character. We have also shuffled the input to the model to add some randomness. Note that buffer size if larger than the dataset size means an ideal situation for random selection. \n",
    "\n",
    "Let's also define the parameters for our network. While converting characters into integers was the first step, these integer IDs are arbitrary and do not truly represent characters and how they should be represented. The Embedding layer will allow us to learn the relationship between characters. This is much better than one-hot encoding. So as part of predicting a sequence of characters, our model will also learn to better represent each character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7305b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters for the network\n",
    "vocab_size = len(chars)   # number of unique characters\n",
    "embedding_dim = 256       # dimensions of character embeddings\n",
    "rnn_units = 512           # LSTM hidden units\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(None,)), # None makes the model general to different sizes of inputs\n",
    "    Embedding(vocab_size, embedding_dim), # add an Embedding layer to convert integer \n",
    "                                          #representation of characters into vector representation\n",
    "    LSTM(rnn_units, return_sequences=True), # return output at each time step\n",
    "    Dropout(0.2),\n",
    "    Dense(vocab_size) # output is a probability distribution across all characters\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True) # sparse categories so apply softmax to get probability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977bf6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "history = model.fit(dataset, epochs=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449236ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6958b9f",
   "metadata": {},
   "source": [
    "Once our network is trained, we can now make predictions from it. We will define a function that takes in a starting sequence and then predicts what the next character should be. That then becomes the input again and we can keep predicting the next character to build sentences. \n",
    "\n",
    "Temperature regulates how conservative or random the prediction should be. Predictions are the raw predictions from the model which are to passed to a softmax function to calculate probabilities. By dividing the logits with temperature, we can change the shape of the probability distribution. \n",
    "- T = 1.0: no change \n",
    "- T < 1.0: model is more predictable/confident - largest logits become more prominent\n",
    "- T > 1.0: model is more random/creative - flattens distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3a813387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,starting_seq,num_generate=1000,temperature=1.0):\n",
    "    input_eval = [char2idx[c] for c in starting_seq.lower()] # convert input chars to ints\n",
    "    input_eval = tf.expand_dims(input_eval, 0)  # add batch dimension for tf\n",
    "\n",
    "    generated_text = [] # here we will store the predicted characters\n",
    "\n",
    "    for i in range(num_generate):\n",
    "        predictions = model.predict(input_eval, verbose=0)\n",
    "        predictions = tf.squeeze(predictions, 0) # remove batch dimension\n",
    "\n",
    "        # apply temperature\n",
    "        predictions = predictions / temperature\n",
    "\n",
    "        # get the predicted character for each time step\n",
    "        # but we only need the very last predicted character (-1)\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        # now this predicted character becomes the new input to make the next prediction\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        generated_text.append(idx2char[predicted_id]) # add prediction to list\n",
    "    \n",
    "    return starting_seq + ''.join(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f8cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model,\"alice \",5,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef73455",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model,\"alice \",5,5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model,\"alice \",5,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, \"alice \", num_generate=1000, temperature=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4584e6b",
   "metadata": {},
   "source": [
    "Let's now train a GRU using the same setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters for the network\n",
    "vocab_size = len(chars)   # number of unique characters\n",
    "embedding_dim = 256       # dimensions of character embeddings\n",
    "rnn_units = 512           # LSTM hidden units\n",
    "\n",
    "# the dropout here is within the GRU layer call\n",
    "# dropout will randomly dropinput features at each time step\n",
    "# recurrent_dropout will randomly drop hidden states from being passed to the next one\n",
    "model = Sequential([\n",
    "    Input(shape=(None,)), # None makes the model general to different sizes of inputs\n",
    "    Embedding(vocab_size, embedding_dim), # add an Embedding layer to convert integer \n",
    "                                          #representation of characters into vector representation\n",
    "    GRU(rnn_units, return_sequences=True,dropout=0.3,recurrent_dropout=0.3), \n",
    "    Dense(vocab_size) # output is a probability distribution across all characters\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True) # sparse categories so apply softmax to get probability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6324b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "history = model.fit(dataset, epochs=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6da48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4552e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model,\"alice \",5,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model,\"alice \",5,5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04611e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model,\"alice \",5,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, \"alice \", num_generate=1000, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ccb1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
